<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>malaysia on Philip&#39;s Curve</title>
    <link>/tags/malaysia/</link>
    <description>Recent content in malaysia on Philip&#39;s Curve</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Aug 2018 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/tags/malaysia/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Exploring GE14 results with Python (part 2)</title>
      <link>/post/exploring-ge14-results-with-python-2/</link>
      <pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/exploring-ge14-results-with-python-2/</guid>
      <description>

&lt;p&gt;See &lt;a href=&#34;https://philip-khor.github.io/post/exploring-ge14-results-with-python/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for part 1.&lt;/p&gt;

&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;

&lt;p&gt;DataTarik used feature importances from random forests to conclude that&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &amp;lsquo;most important&amp;rsquo; ethnic composition appears to be Chinese&lt;/li&gt;
&lt;li&gt;Ethnicity as a whole is related to election outcomes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I argue here that demography considered jointly has a greater relationship with election outcomes.&lt;/p&gt;

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Here are the feature variables expressed as a correlation matrix and visualised in a heatmap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/output_51_1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Using the &lt;code&gt;feature_importances&lt;/code&gt; implementation in &lt;code&gt;scikit-learn&lt;/code&gt;, it&amp;rsquo;s unclear what&amp;rsquo;s going on when each feature tends to have strong associations with other features. Specifically, one feature tends to be the complement of another. If the algorithm tells that the proportion of Chinese is more important, perhaps the proportion of Chinese is inversely related with the proportion of Malays. If that is so, how do we tell which feature matters? Trying to interpret each feature as it is would be futile.&lt;/p&gt;

&lt;p&gt;As discussed in &lt;a href=&#34;http://explained.ai/rf-importance/i&#34; target=&#34;_blank&#34;&gt;explained.ai&amp;rsquo;s blog post (Beware Default Random Forest Importances)&lt;/a&gt;, the default implementation of feature importance in &lt;code&gt;scikit-learn&lt;/code&gt; is biased and susceptible to collinearity. More importantly, the features are almost perfectly collinear within each &amp;lsquo;meta-group&amp;rsquo; of features. Many of the age and race variables sum to 100%.&lt;/p&gt;

&lt;p&gt;Here is a plot of the distribution of the sum of age and race variables in the entire dataset.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/output_52_0.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To deal with the collinearity, Terence Parr, Kerem Turgutlu, Christopher Csiszar, and Jeremy Howard suggest to use drop-column importance in conjunction with the permutation importances metric. In contrast with the default feature importance metric, which measures the mean decrease in impurity, the permutation score permutates a column and then calculates the drop in the score from the baseline score. Do the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;compute baseline feature importance with permutation importance&lt;/li&gt;
&lt;li&gt;drop a column, retrain, and then recompute feature importance scores&lt;/li&gt;
&lt;li&gt;The importance score for a column is the difference between the baseline and the score for the model missing that column&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;drop-column&#34;&gt;Drop-column&lt;/h3&gt;

&lt;p&gt;The feature importances from drop-column importances are shown as follows. These are computed using Terence Parr and Kerem Turgutlu&amp;rsquo;s &lt;code&gt;rfpimp&lt;/code&gt; package:&lt;br /&gt;
&lt;img src=&#34;/img/output_55_1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now, race matters but age doesn&amp;rsquo;t matter. Age has a negative permutation score - apparently permuting the age columns &lt;em&gt;improves&lt;/em&gt; the model&amp;rsquo;s performance?&lt;/p&gt;

&lt;h3 id=&#34;meta-features&#34;&gt;Meta-features&lt;/h3&gt;

&lt;p&gt;A meta-features approach using permutation importance is another approach that can be taken here. &lt;code&gt;rfpimp&lt;/code&gt; also provides an implementation of drop-column importances. The results are shown as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/output_60_0.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Over 10 runs, the ratio of feature importance of the race- to age-meta-feature was:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[3.2307692307692304,
 2.4666666666666672,
 2.294117647058823,
 3.2307692307692304,
 2.058823529411764,
 1.9999999999999996,
 2.117647058823529,
 3.3333333333333335,
 2.8000000000000007,
 2.235294117647059]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This confirms our result from drop-column that race matters here, but age doesn&amp;rsquo;t &lt;em&gt;not&lt;/em&gt; matter.&lt;/p&gt;

&lt;p&gt;But what if I consider all of them jointly? The permutation score skyrockets:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/output_61_0.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;and the permutation score over 10 runs is&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[0.3932584269662921, 
 0.38202247191011235,
 0.4044943820224719,
 0.3033707865168539,
 0.3595505617977528,
 0.2134831460674157,
 0.348314606741573,
 0.3146067415730337,
 0.2584269662921348,
 0.348314606741573]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which is far higher than the permutation scores from age- and race-meta-features.&lt;/p&gt;

&lt;h2 id=&#34;decision-trees&#34;&gt;Decision trees&lt;/h2&gt;

&lt;p&gt;To get a more interpretable model, we can use a decision tree model to fit the data. Using &lt;code&gt;scikit-learn&lt;/code&gt;&amp;rsquo;s implementation of &lt;code&gt;DecisionTreeClassifier&lt;/code&gt;, I get 71.2% accuracy on a test set of 40%. The decision tree can be found &lt;a href=&#34;https://www.dropbox.com/s/hgn5dno60trmbf0/myTree.pdf?dl=1&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. However, I don&amp;rsquo;t think it&amp;rsquo;s useful to answer our question either - it&amp;rsquo;s quite a complex tree, and it can&amp;rsquo;t give us a straight answer about whether age or race - or which aspects of age and race - are more important.&lt;/p&gt;

&lt;h2 id=&#34;pca-and-demographic-patterns&#34;&gt;PCA and demographic patterns&lt;/h2&gt;

&lt;p&gt;There is a potential use case for principal component analysis (PCA) here. Perhaps PCA can identify demographic patterns among the age and race variables. The figure below shows the ethnic breakdown from an inverse transformation of the first principal component. This is done by first transforming the ethnic variables by &lt;code&gt;scikit-learn&lt;/code&gt;&amp;rsquo;s &lt;code&gt;PCA&lt;/code&gt; implementation, then using &lt;code&gt;scikit-learn&lt;/code&gt;&amp;rsquo;s &lt;code&gt;inverse_transform&lt;/code&gt; function to transform the data back to its original space. This gives us an idea of what the first principal component is picking up in the data.&lt;/p&gt;

&lt;p&gt;I haven&amp;rsquo;t investigated the data exhaustively, so I&amp;rsquo;ve picked 5 random numbers within the range of the first principal component and sorted them to have some sense of representativeness. It looks like the first principal component captures a spectrum of constituency groups from high-Chinese constituencies to high-Sabahan constituencies. Note that the first principal component only captures 28% of the variance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/output_97_1.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;/img/output_97_3.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;/img/output_97_5.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;/img/output_97_7.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;/img/output_97_9.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The same exercise is done for the age variables. Its first principal component captures 61.4% of the variance. For age, PCA has an easier job - it just sorts between constituencies with left-skewed and right-skewed age distributions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/output_99_1.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;/img/output_99_3.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;/img/output_99_5.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;/img/output_99_7.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;/img/output_99_9.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;multinomial-logit-pca&#34;&gt;Multinomial logit + PCA&lt;/h2&gt;

&lt;p&gt;We can see how these patterns identified via PCA affect electoral outcomes using a multinomial logit model. A multinomial logit allows us to obtain relative and marginal probabilities of each class, so that we can interpret the model for each contesting party. Note that PCA is not the best approach for this. Lubostky and Wittenberg (2002) note that it is not clear that the PCA procedure helps with capturing the structural relationship between a latent variable and an outcome of interest.&lt;/p&gt;

&lt;p&gt;(I estimated my multinomial logit on a 70% split of the data. This is an arbitrary decision, but since prediction is not the core task of this model, I&amp;rsquo;m not too interested in evaluating its predictive power.)&lt;/p&gt;

&lt;p&gt;Using &lt;code&gt;statsmodels&lt;/code&gt;&amp;rsquo;s &lt;code&gt;MNLogit&lt;/code&gt;, the regression statistics are as follows. This model predicts with 16% accuracy out-of-sample.
&lt;!--html_preserve--&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;MNLogit Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;         &lt;td&gt;y&lt;/td&gt;        &lt;th&gt;  No. Observations:  &lt;/th&gt;  &lt;td&gt;   155&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;              &lt;td&gt;MNLogit&lt;/td&gt;     &lt;th&gt;  Df Residuals:      &lt;/th&gt;  &lt;td&gt;   147&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;               &lt;td&gt;MLE&lt;/td&gt;       &lt;th&gt;  Df Model:          &lt;/th&gt;  &lt;td&gt;     4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;          &lt;td&gt;Sun, 26 Aug 2018&lt;/td&gt; &lt;th&gt;  Pseudo R-squ.:     &lt;/th&gt;  &lt;td&gt;-0.2011&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;              &lt;td&gt;15:34:15&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -203.94&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;converged:&lt;/th&gt;           &lt;td&gt;True&lt;/td&gt;       &lt;th&gt;  LL-Null:           &lt;/th&gt; &lt;td&gt; -169.80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt; &lt;/th&gt;                      &lt;td&gt; &lt;/td&gt;        &lt;th&gt;  LLR p-value:       &lt;/th&gt;  &lt;td&gt; 1.000&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;th&gt;y=BN&lt;/th&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&amp;gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;&lt;br /&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Race&lt;/th&gt;                &lt;td&gt;    0.3883&lt;/td&gt; &lt;td&gt;    0.211&lt;/td&gt; &lt;td&gt;    1.838&lt;/td&gt; &lt;td&gt; 0.066&lt;/td&gt; &lt;td&gt;   -0.026&lt;/td&gt; &lt;td&gt;    0.802&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Age&lt;/th&gt;                 &lt;td&gt;   -0.3440&lt;/td&gt; &lt;td&gt;    0.151&lt;/td&gt; &lt;td&gt;   -2.275&lt;/td&gt; &lt;td&gt; 0.023&lt;/td&gt; &lt;td&gt;   -0.640&lt;/td&gt; &lt;td&gt;   -0.048&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;y=Gagasan Sejahtera&lt;/th&gt;    &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&amp;gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;&lt;br /&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Race&lt;/th&gt;                &lt;td&gt;    0.0476&lt;/td&gt; &lt;td&gt;    0.240&lt;/td&gt; &lt;td&gt;    0.198&lt;/td&gt; &lt;td&gt; 0.843&lt;/td&gt; &lt;td&gt;   -0.424&lt;/td&gt; &lt;td&gt;    0.519&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Age&lt;/th&gt;                 &lt;td&gt;   -0.3428&lt;/td&gt; &lt;td&gt;    0.154&lt;/td&gt; &lt;td&gt;   -2.222&lt;/td&gt; &lt;td&gt; 0.026&lt;/td&gt; &lt;td&gt;   -0.645&lt;/td&gt; &lt;td&gt;   -0.040&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;y=PH&lt;/th&gt;    &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&amp;gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;&lt;br /&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Race&lt;/th&gt; &lt;td&gt;   -1.4327&lt;/td&gt; &lt;td&gt;    0.275&lt;/td&gt; &lt;td&gt;   -5.216&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.971&lt;/td&gt; &lt;td&gt;   -0.894&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Age&lt;/th&gt;  &lt;td&gt;    0.3323&lt;/td&gt; &lt;td&gt;    0.150&lt;/td&gt; &lt;td&gt;    2.216&lt;/td&gt; &lt;td&gt; 0.027&lt;/td&gt; &lt;td&gt;    0.038&lt;/td&gt; &lt;td&gt;    0.626&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;y=WARISAN&lt;/th&gt;    &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&amp;gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;&lt;br /&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Race&lt;/th&gt;      &lt;td&gt;    0.3364&lt;/td&gt; &lt;td&gt;    0.215&lt;/td&gt; &lt;td&gt;    1.568&lt;/td&gt; &lt;td&gt; 0.117&lt;/td&gt; &lt;td&gt;   -0.084&lt;/td&gt; &lt;td&gt;    0.757&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Age&lt;/th&gt;       &lt;td&gt;   -0.2115&lt;/td&gt; &lt;td&gt;    0.153&lt;/td&gt; &lt;td&gt;   -1.380&lt;/td&gt; &lt;td&gt; 0.167&lt;/td&gt; &lt;td&gt;   -0.512&lt;/td&gt; &lt;td&gt;    0.089&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;!--/html_preserve--&gt;&lt;/p&gt;

&lt;h3 id=&#34;relative-odds&#34;&gt;Relative odds&lt;/h3&gt;

&lt;p&gt;The coefficients are expressed in log-odds, so these are exponentiated to obtain relative odds. The relative odds can be interpreted as the probability &lt;strong&gt;relative to BEBAS&lt;/strong&gt; (independent candidate)  being improved by the relative odds if Race/Age increased by one standard deviation.&lt;/p&gt;

&lt;p&gt;&lt;!--html_preserve--&gt;&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;BN&lt;/th&gt;
      &lt;th&gt;Gagasan Sejahtera&lt;/th&gt;
      &lt;th&gt;PH&lt;/th&gt;
      &lt;th&gt;WARISAN&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Race&lt;/th&gt;
      &lt;td&gt;1.474485&lt;/td&gt;
      &lt;td&gt;1.048732&lt;/td&gt;
      &lt;td&gt;0.238672&lt;/td&gt;
      &lt;td&gt;1.399909&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;td&gt;0.708961&lt;/td&gt;
      &lt;td&gt;0.709755&lt;/td&gt;
      &lt;td&gt;1.394134&lt;/td&gt;
      &lt;td&gt;0.809366&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;&lt;!--/html_preserve--&gt;&lt;/p&gt;

&lt;h3 id=&#34;marginal-odds&#34;&gt;Marginal odds&lt;/h3&gt;

&lt;p&gt;We can tell a slightly different story with marginal odds. The advantage of using marginal odds is that we can get an idea of actual, rather than relative probabilities (see &lt;a href=&#34;http://data.princeton.edu/wws509/stata/mlogit.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for details), however they can be quite tricky to interpret. The marginal effect for a continuous variable &lt;code&gt;$X_k$&lt;/code&gt; provides the instantaneous rate of change if the variable &lt;code&gt;$X_k$&lt;/code&gt; increased by an infinitesimal amount &lt;code&gt;$\Delta$&lt;/code&gt;, holding all other variables &lt;code&gt;$X$&lt;/code&gt; constant. See &lt;a href=&#34;https://www3.nd.edu/~rwilliam/xsoc73994/Margins02.pdf&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; for details.
&lt;code&gt;$$\lim_{\Delta \to 0} \Pr(Y = 1|X, X_k+\Delta)- \Pr(y=1|X, X_k)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In other words, what is the change in (log-)probabiity if the first principal component changes by an infinitesimal amount?&lt;/p&gt;

&lt;p&gt;The means of each principal component will coincide with 0, therefore at 0 I get marginal effects at average race/age. This is visualised below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/output_82_0.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;lubotsky-wittenberg-post-hoc-estimator-for-multiple-proxies&#34;&gt;Lubotsky-Wittenberg post-hoc estimator for multiple proxies&lt;/h1&gt;

&lt;p&gt;Lubotsky and Wittenberg (2002) proposed a method to interpret the coefficients in a regression under the null hypothesis that the variables are all generated by a common latent factor. Let the latent factor &lt;code&gt;$x_{i}^*$&lt;/code&gt; be measured by &lt;code&gt;$j$&lt;/code&gt; proxy variables &lt;code&gt;$z_{ji}$&lt;/code&gt;, where each &lt;code&gt;$z_{ji}$&lt;/code&gt; is related to the latent factor by &lt;code&gt;$$z_{ji}=\rho x^*_i + \mu _{ij}$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The goal is to measure the effect of &lt;code&gt;$x^*$&lt;/code&gt; on &lt;code&gt;$y$&lt;/code&gt;: &lt;code&gt;$$y_i = \beta_{LW} x^*_{it}+\epsilon_i$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I follow the estimation procedure in Vosters &amp;amp; Nybom (2016):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Fit the regression model: &lt;code&gt;$$y_i=\phi_1 z_{1i} + \phi_2 z_{2i} + ...$$&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Estimate &lt;code&gt;$\rho_j$&lt;/code&gt; of each &lt;code&gt;$z_j$&lt;/code&gt; (except &lt;code&gt;$z_1$&lt;/code&gt;, where &lt;code&gt;$\rho_j$&lt;/code&gt; is normalised to 1) using two-stage least squares. The procedure is akin to instrumental variables: use &lt;code&gt;$z_j$&lt;/code&gt; as outcome and  &lt;code&gt;$y$&lt;/code&gt; as instrument for &lt;code&gt;$z_1$&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Calculate &lt;code&gt;$$\beta_{LW}=\rho_1\phi_1 + \rho_2 \phi_2 + \rho_3 \phi_3 + ... + \rho_k \phi_k$$&lt;/code&gt;
where &lt;code&gt;$\rho_1 = 1$&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To be on the safe side, I assume a linear probability model, where the outcome of interest is whether Pakatan Harapan wins the seat. I&amp;rsquo;m not too familiar with the literature, but I couldn&amp;rsquo;t really find it being used in a classification problem before, so I&amp;rsquo;m not sure if it&amp;rsquo;s shown to be appropriate for the logistic model.&lt;/p&gt;

&lt;p&gt;And since I can&amp;rsquo;t figure out how to properly construct my standard errors, you&amp;rsquo;ll just have to make do with estimates.&lt;/p&gt;

&lt;p&gt;The F-stat is provided for the first stage: a rule of thumb is to have an F-stat which exceeds 10. The F-stat tests whether the instrument is &amp;lsquo;relevant&amp;rsquo;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I write two functions: &lt;code&gt;lw&lt;/code&gt; and &lt;code&gt;lw_controls&lt;/code&gt;: one function provides the L-W estimate without controls, and the other with controls.&lt;/li&gt;
&lt;li&gt;My training set is similar to the previous training set for the multinomial logit.&lt;/li&gt;
&lt;li&gt;I start out estimating without controls - this gives me extremely small effects. If Race is independent of Age (e.g. diagram below), the estimation is unbiased.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&amp;nbsp;&lt;/th&gt;
&lt;th&gt;Race&lt;/th&gt;
&lt;th&gt;Age&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;F-stat&lt;/td&gt;
&lt;td&gt;65.430&lt;/td&gt;
&lt;td&gt;119.093&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$\hat \beta_{LW}$&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;.00073&lt;/td&gt;
&lt;td&gt;-.00053&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&#34;/img/dagitty-model.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Estimating the effect of &amp;lsquo;race&amp;rsquo; conditional on age (and vice versa) gives me larger effects. This suggests that there&amp;rsquo;s attenuation bias from omitting controls. These are &amp;lsquo;good&amp;rsquo; estimates if race confounds age, or age confounds race.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&amp;nbsp;&lt;/th&gt;
&lt;th&gt;Race&lt;/th&gt;
&lt;th&gt;Age&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;F-stat&lt;/td&gt;
&lt;td&gt;32.288&lt;/td&gt;
&lt;td&gt;96.985&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$\hat \beta_{LW}$&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-0.041&lt;/td&gt;
&lt;td&gt;0.072&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;However, it&amp;rsquo;s not clear that demographic factors should be considered separately. Putting race and ethnicity variables together, the estimated L-W effect spikes.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&amp;nbsp;&lt;/th&gt;
&lt;th&gt;Demography&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;F-stat&lt;/td&gt;
&lt;td&gt;119.093&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$\hat \beta_{LW}$&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;0.971&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Interestingly, this result seems to confirm what we&amp;rsquo;ve seen in the random forests feature importances scores.&lt;/p&gt;

&lt;p&gt;This estimate is OK assuming there are no confounders for the relationship between demography and electoral outcomes. For example, demography could be confounded by economic conditions that affect both the demographies of the region and preferences of the electorate.&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;Interpreting feature importance metrics should be done with caution, particularly with random forests. The results suggest that there is interaction within and between age and ethnicity variables that contributes to electoral performance. The way I think of it, there are age-race combinations that are important contributors to electoral performance. Random Forests help with detecting interactions, however it&amp;rsquo;s not clear how these can be disentangled in feature importances metrics.&lt;/p&gt;

&lt;p&gt;Also, the effect of demography could be reflective of unmeasured confounders. Without carefully considering the determinants of electoral performance, it&amp;rsquo;s not clear if demography is of such critical importance to electoral outcomes.&lt;/p&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I started out being critical with the 10% test set (23 observations). But the author of the original article set out to do estimation and not prediction, so I don&amp;rsquo;t think it&amp;rsquo;s too big a deal. In any case, I used test sets between 30 and 40% for this article.&lt;/li&gt;
&lt;li&gt;The data is scaled before it is transformed by PCA, and the data is inverse-transformed by the original scaler before constructing these charts. Variables with negative inverse-transformed values are set to zero.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Exploring GE14 results with Python (part 1)</title>
      <link>/post/exploring-ge14-results-with-python/</link>
      <pubDate>Sat, 25 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/exploring-ge14-results-with-python/</guid>
      <description>

&lt;p&gt;There&amp;rsquo;s an interesting blog post up on &lt;a href=&#34;https://datatarik.com/home/2018/8/21/exploring-ge14-with-data-science-part-1&#34; target=&#34;_blank&#34;&gt;DataTarik&lt;/a&gt; about using random forests to model the outcomes of Malaysia&amp;rsquo;s 14th general election (GE14). While I have my reservations about their machine learning approach, I think it&amp;rsquo;s an interesting dataset, so I downloaded their &lt;a href=&#34;https://github.com/khoo-j/MsiaGE14/blob/master/GE14_Age-Ethnicity-bySeats.xlsx&#34; target=&#34;_blank&#34;&gt;data&lt;/a&gt; and &lt;a href=&#34;https://github.com/khoo-j/MsiaGE14/blob/master/Random_Forest_2018-Age%20and%20Ethnicity.ipynb&#34; target=&#34;_blank&#34;&gt;code&lt;/a&gt; and played around with it in Python.&lt;/p&gt;

&lt;p&gt;For those not familiar with Malaysia, some background:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;BN refers to the National Front (Barisan Nasional), the current largest opposition party and the ruling party since independence, and PH refers to Pact of Hope (Pakatan Harapan), the party currently in government. I sometimes refer to Gagasan Sejahtera as PAS (Malaysian Islamic Party), as they were the leading party in this electoral coalition.&lt;/li&gt;
&lt;li&gt;Melayu, Cina and India refer to ethnic Malays, Chinese and Indians respectively, whereas Bumiputera Sabah/Sarawak and Orang Asli are various indigenous tribes in Malaysia. Malays are also regarded as indigenous, and will be referred to as such.&lt;/li&gt;
&lt;li&gt;Geographically, Malaysia comprises of parts of the Malay Peninsula and Borneo island (West and East Malaysia respectively).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Variable names&lt;/strong&gt;: &lt;code&gt;agevars&lt;/code&gt; refers to proportions of people in several bins of age groups, &lt;code&gt;racevars&lt;/code&gt; refers to proportions of people in each ethnic group, and &lt;code&gt;features&lt;/code&gt; is the union of &lt;code&gt;agevars&lt;/code&gt; and &lt;code&gt;racevars&lt;/code&gt;. &lt;code&gt;target&lt;/code&gt; is the outcome of the election.&lt;/p&gt;

&lt;h1 id=&#34;who-won&#34;&gt;Who won?&lt;/h1&gt;

&lt;p&gt;Thought I&amp;rsquo;d just do a quick waffle chart using the sweet &lt;code&gt;pywaffle&lt;/code&gt; package.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pywaffle import Waffle
data = ge14.target.value_counts()
fig = plt.figure(
    FigureClass=Waffle, 
    rows=10, 
    values=data, 
    colors=(&amp;quot;red&amp;quot;, &amp;quot;#232066&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;lightblue&amp;quot;, &#39;grey&#39;, &#39;cyan&#39;),
    title={&#39;label&#39;: &#39;14th Malaysian Parliament composition&#39;, &#39;loc&#39;: &#39;left&#39;},
    labels=[&amp;quot;{0}&amp;quot;.format(k) for k, v in data.items()],
    icons = &#39;user&#39;,
    legend={&#39;loc&#39;: &#39;upper left&#39;, &#39;bbox_to_anchor&#39;: (1, 1)}
)
fig.gca().set_facecolor(&#39;#EEEEEE&#39;)
fig.set_facecolor(&#39;#EEEEEE&#39;)
fig.set_tight_layout(False)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/img/output_6_0.png&#34; alt=&#34;waffle&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;overall-distribution-of-demographics-across-constituencies&#34;&gt;Overall distribution of demographics across constituencies&lt;/h1&gt;

&lt;p&gt;With the data we have, there are two stories to be told:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Voting behaviour by ethnicity&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;Gagasan Sejahtera does well in constituencies with high Malay concentration, low Chinese and Indian concentration.&lt;/li&gt;
&lt;li&gt;We may want to look at geographical subgroups (Peninsula/Borneo), since the Borneo parties don&amp;rsquo;t contest in the Peninsula, and some of the ethnic groups covered are sparse in the Peninsula.&lt;/li&gt;
&lt;li&gt;For BN, the distribution of wins by ethnicity are more tightly distributed around low Indian and Chinese population constituencies.&lt;/li&gt;
&lt;li&gt;Both BN and PH have wins across a range of Malay-minority and Malay-majority constituencies. BN does not have any seats in Chinese-majority constituencies.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Voting behaviour by age&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Not very clear pattern for age - at least not with the boxplot&lt;/li&gt;
&lt;li&gt;PAS wins in &amp;lsquo;younger&amp;rsquo; constituencies (higher pct 21-30, 31-40)&lt;/li&gt;

&lt;li&gt;&lt;p&gt;PAS wins are very uniform in terms of age and ethnicity demographics&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fg, ax = plt.subplots(nrows =4, figsize = (6,20))
ge14[ge14.target ==&#39;BN&#39;][features].boxplot(vert = False, ax = ax[0])
ax[0].set_title(&amp;quot;BN&amp;quot;)
ge14[ge14.target==&#39;PH&#39;][features].boxplot(vert = False, ax = ax[1])
ax[1].set_title(&amp;quot;PH&amp;quot;)
ge14[ge14.target==&#39;Gagasan Sejahtera&#39;][features].boxplot(vert = False, ax = ax[2])
ax[2].set_title(&amp;quot;Gagasan Sejahtera&amp;quot;)
ge14[~ge14.target.isin([&#39;BN&#39;, &#39;PH&#39;, &#39;Gagasan Sejahtera&#39;])][features].boxplot(vert = False, ax = ax[3])
ax[3].set_title(&amp;quot;Others&amp;quot;)

plt.tight_layout()
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/img/output_8_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;in-which-seats-do-these-parties-win&#34;&gt;In which seats do these parties win?&lt;/h1&gt;

&lt;h2 id=&#34;ethnic-diversity-of-constituencies&#34;&gt;Ethnic diversity of constituencies&lt;/h2&gt;

&lt;p&gt;Using &lt;code&gt;scipy&lt;/code&gt;&amp;rsquo;s &lt;code&gt;entropy&lt;/code&gt; function, I try to determine which parties win in ethnically &amp;lsquo;diverse&amp;rsquo; and &amp;lsquo;less-diverse&amp;rsquo; constituencies. The boxplots below confirm what we already know from the boxplots above - that Gagasan Sejahtera appeals to a very homogenous group of voters.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;BN wins across diverse and non-diverse constituencies;&lt;/li&gt;
&lt;li&gt;PH also wins across diverse and non-diverse constituencies, but its median constituency is more diverse than BN&amp;rsquo;s median constituency.&lt;/li&gt;
&lt;li&gt;PAS wins in non-diverse constituencies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The diversity index used here is the Shannon entropy index, given by &lt;code&gt;$$H&#39;=-\Sigma p_i \log{p_i}$$&lt;/code&gt; The exponent of &lt;code&gt;$H&#39;$&lt;/code&gt; has a nice interpretation. It is the effective number of species, i.e. how many species there would be if each species had an equal amount of observations. You can convince yourself that this is true by running the code &lt;code&gt;exp(-(0.5*log(0.5)+0.5*log(0.5)))&lt;/code&gt; (in R), which should return the result 2, meaning the diversity in the data is effectively two equally sized classes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import entropy
entropy_const = np.array([entropy(ge14[racevars].iloc[i,:].tolist()) for i in range(len(ge14))])

# use effective number of species instead https://medium.com/@sam.weinger/how-diverse-are-names-in-america-f74b07e031bd
effective_number = np.exp(entropy_const)

np.mean(effective_number[ge14.target==&#39;BN&#39;]), np.mean(effective_number[ge14.target==&#39;PH&#39;]), np.mean(effective_number[ge14.target==&#39;Gagasan Sejahtera&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(2.023986123800037, 2.4850578347860295, 1.2839901761045327)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fg, ax = plt.subplots(nrows = 3, figsize = (5,5))
sns.boxplot(effective_number[ge14.target==&#39;BN&#39;], ax = ax[0])
ax[0].set_title(&amp;quot;BN&amp;quot;)

sns.boxplot(effective_number[ge14.target==&#39;PH&#39;], ax = ax[1], color = &#39;r&#39;)
ax[1].set_title(&amp;quot;PH&amp;quot;)
sns.boxplot(effective_number[ge14.target==&#39;Gagasan Sejahtera&#39;], ax = ax[2], color = &#39;g&#39;)
ax[2].set_title(&amp;quot;Gagasan Sejahtera&amp;quot;)
plt.xlim(0,4)
plt.tight_layout()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/img/output_17_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;peninsula-borneo-divide&#34;&gt;Peninsula-Borneo divide?&lt;/h1&gt;

&lt;p&gt;Since there are different parties in Peninsula and Borneo, I calculate age-ethnicity cross-tabulations for Peninsula and Borneo respectively. Age and ethnicity here refers to the demographic with the highest share in each constituency.&lt;/p&gt;

&lt;h2 id=&#34;peninsula&#34;&gt;Peninsula&lt;/h2&gt;

&lt;p&gt;PH wins all predominantly ethnic Chinese seats and most predominantly ethnic Malay seats in the Peninsula.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ethnicity = ge14[racevars].idxmax(axis = 1)
age = ge14[agevars].idxmax(axis = 1)

pd.crosstab(ge14[peninsula].target, ethnicity)
&lt;/code&gt;&lt;/pre&gt;

&lt;!--html_preserve--&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: center;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: center;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Cina (%)&lt;/th&gt;
      &lt;th&gt;Melayu (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;BN&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;49&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Gagasan Sejahtera&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;PH&lt;/th&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;65&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;!--/html_preserve--&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.crosstab(ge14[peninsula].target, age)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;!--html_preserve--&gt;&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: center;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: center;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;21 - 30 (%)&lt;/th&gt;
      &lt;th&gt;31 - 40 (%)&lt;/th&gt;
      &lt;th&gt;41 - 50 (%)&lt;/th&gt;
      &lt;th&gt;51 - 60 (%)&lt;/th&gt;
    &lt;/tr&gt;&lt;/p&gt;

&lt;p&gt;&lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;BN&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;47&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Gagasan Sejahtera&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;PH&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;77&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;&lt;!--/html_preserve--&gt;&lt;/p&gt;

&lt;h1 id=&#34;borneo&#34;&gt;Borneo&lt;/h1&gt;

&lt;p&gt;BN is strongest in predominantly-indigenous seats, whereas PH is strongest in predominantly ethnic Chinese seats. PH made up for its weakness in indigenous seats with its partnership with WARISAN. Both BN and PH win the most seats where most people are aged between 31 and 40.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.crosstab(ge14[np.invert(peninsula)].target, ethnicity)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;!--html_preserve--&gt;&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: center;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: center;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Bumiputera Sabah (%)&lt;/th&gt;
      &lt;th&gt;Bumiputera Sarawak (%)&lt;/th&gt;
      &lt;th&gt;Cina (%)&lt;/th&gt;
      &lt;th&gt;Melayu (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;BEBAS&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BN&lt;/th&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;PH&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;STAR&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;WARISAN&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;!--/html_preserve--&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.crosstab(ge14[np.invert(peninsula)].target, age)
&lt;/code&gt;&lt;/pre&gt;

&lt;!--html_preserve--&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: center;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: center;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;31 - 40 (%)&lt;/th&gt;
      &lt;th&gt;41 - 50 (%)&lt;/th&gt;
      &lt;th&gt;51 - 60 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;BEBAS&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BN&lt;/th&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;PH&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;STAR&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;WARISAN&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;!--/html_preserve--&gt;
</description>
    </item>
    
    <item>
      <title>Visualising government expenditure data with ggplot2</title>
      <link>/post/visualising-government-expenditure-dat/</link>
      <pubDate>Fri, 24 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/visualising-government-expenditure-dat/</guid>
      <description>


&lt;p&gt;I thought I’d practice my rusty R skills when I came across a post by Khairil Yusof on the Sinar Project Facebook group that Malaysian Administrative Modernisation and Management Planning Unit (MAMPU) has made data on development expenditure over time available on data.gov.my. I haven’t paid close enough attention to government budgets to have much comments about the trends, but a comment can be found &lt;a href=&#34;https://penanginstitute.org/programmes/penang-institute-in-kuala-lumpur/1038-analysing-malaysias-development-expenditure-in-the-federal-budget-from-2004-to-2018/&#34;&gt;here&lt;/a&gt;. Tl;dr, the authors note that Malaysia’s stagnating development expenditure is concerning since Malaysia is a developing economy.&lt;/p&gt;
&lt;p&gt;My thinking while constructing this plot was to contrast the growth in each category of expenditure against the overall growth in development expenditure over time. I’m thinking about also including the trend in each category of expenditure within each facet.&lt;/p&gt;
&lt;p&gt;Data is obtained from &lt;a href=&#34;http://www.data.gov.my/data/ms_MY/dataset/perbelanjaan-pembangunan-kerajaan-persekutuan/resource/7a5c73ab-7496-4aa6-a924-a1d0e1fff53d&#34;&gt;data.gov.my&lt;/a&gt;. A cleaned version of the data is available &lt;a href=&#34;https://www.dropbox.com/s/m546u01hsf8zvb2/malaysiaDevExp.csv?dl=1&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
library(tidyverse)
library(here)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;malaysiaDevExp &amp;lt;- read_csv(here(&amp;quot;static&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;malaysiaDevExp.csv&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   Year = col_double(),
##   EconomyTotal = col_double(),
##   AgroRuralDev = col_double(),
##   PublicAmenities = col_double(),
##   TradeAndIndustry = col_double(),
##   Transport = col_double(),
##   Connectivity = col_double(),
##   Environment = col_double(),
##   OtherEconomy = col_double(),
##   SocialTotal = col_double(),
##   EducationTraining = col_double(),
##   Health = col_double(),
##   Housing = col_double(),
##   OtherSocial = col_double(),
##   SafetyTotal = col_double(),
##   Defence = col_double(),
##   HomelandSecurity = col_double(),
##   PublicAdmin = col_double(),
##   Total = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create separate dataframe for background totals
malaysiaDevExp %&amp;gt;% 
  select(Year, Value = Total) %&amp;gt;% 
  mutate(Year = as.numeric(Year), 
         Value = Value/1000)-&amp;gt; DevExpTotal

malaysiaDevExp %&amp;gt;%
  # select some columns; rename as needed
  select(Year, 
         Economy = EconomyTotal, 
         Social = SocialTotal, 
         Safety = SafetyTotal, 
         `Public Administration` = PublicAdmin) %&amp;gt;%
  # Year was imported as factor; change this to numeric
  mutate(Year = as.numeric(Year)) %&amp;gt;% 
  gather(ExpCat, Value, -Year) %&amp;gt;% 
  ggplot(aes(x = Year, y = Value/1000)) + 
  # totals in background
  geom_ribbon(data = DevExpTotal, 
              aes(ymin=0, ymax = Value), 
              fill = &amp;#39;gray96&amp;#39;, 
              group = 1) + 
  # line representing each expenditure category, in different colors
  geom_path(group=1, aes(color = ExpCat)) + 
  # separate each expenditure category into facets
  facet_wrap(~ExpCat) + 
  # no legends
  guides(color = FALSE) + 
  # set break points in x-axis 
  scale_x_continuous(breaks = c(1975, 1985, 1995, 2005,2015)) +
  # draw rectangle to highlight that numbers should be treated with caution
  geom_rect(inherit.aes=FALSE, 
            aes(xmin=2017, xmax=2018, ymin=0, ymax=60), 
            color=&amp;quot;transparent&amp;quot;, 
            fill=&amp;quot;lightcoral&amp;quot;, 
            alpha = 0.01) +
  theme_minimal() + 
  theme(panel.grid.major = element_blank(), # remove grid lines
        panel.grid.minor = element_blank(), 
        # bold facet titles
        strip.text = element_text(face = &amp;#39;bold&amp;#39;, color = &amp;#39;gray29&amp;#39;), 
        # format text on ticks
        axis.text = element_text(size = 9, color = &amp;#39;gray45&amp;#39;), 
        # format axis labels
        axis.title = element_text(color = &amp;#39;gray35&amp;#39;),
        # format caption text
        plot.caption = element_text(color = &amp;#39;gray30&amp;#39;)) + 
  labs(title = &amp;quot;Malaysia&amp;#39;s spending on safety and public admin hasn&amp;#39;t grown much&amp;quot;, 
       subtitle = &amp;quot;Strongest growth in development expenditure is in economic and social spending&amp;quot;, 
       y = &amp;quot;RM (&amp;#39;000) million&amp;quot;, 
       caption = &amp;quot;Source: data.gov.my \n2017 data is preliminary data; 2018 data are Budget estimates&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-23-visualising-government-expenditure-data-with-ggplot2_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
