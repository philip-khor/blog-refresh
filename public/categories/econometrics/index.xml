<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>econometrics on Philip&#39;s Curve</title>
    <link>/categories/econometrics/</link>
    <description>Recent content in econometrics on Philip&#39;s Curve</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/categories/econometrics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PSA: Python, OLS and perfectly collinear variables</title>
      <link>/post/psa-python-ols-and-perfectly-collinear-variables/</link>
      <pubDate>Wed, 05 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/psa-python-ols-and-perfectly-collinear-variables/</guid>
      <description>&lt;p&gt;Unlike most implementations of linear models (e.g. Stata, R), Python packages don&amp;rsquo;t usually drop perfectly collinear variables.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s Statsmodels as a first example: (see &lt;a href=&#34;https://github.com/statsmodels/statsmodels/issues/3824&#34; target=&#34;_blank&#34;&gt;https://github.com/statsmodels/statsmodels/issues/3824&lt;/a&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np 
import statsmodels.formula.api as smf
import pandas as pd 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;e = np.random.normal(size = 30)

# creating two variables x and collinear 
# where collinear is just 2 times x
x1 = np.arange(30)
x2 = 2 * x1

y = 2 * x1 + e
data = pd.DataFrame({&amp;quot;y&amp;quot;: y, &amp;quot;x1&amp;quot;: x1, &amp;quot;x2&amp;quot;: x2})
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = smf.ols(&amp;quot;y ~ x1 + x2&amp;quot;, data = data)
res = model.fit()
res.summary()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;            &lt;td&gt;y&lt;/td&gt;        &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.998&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.998&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;1.354e+04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Tue, 04 Jun 2019&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;3.80e-39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;20:23:08&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -35.185&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    30&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   74.37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    28&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   77.17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     1&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;&lt;br /&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;&lt;br /&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&amp;gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;&lt;br /&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    0.3681&lt;/td&gt; &lt;td&gt;    0.288&lt;/td&gt; &lt;td&gt;    1.277&lt;/td&gt; &lt;td&gt; 0.212&lt;/td&gt; &lt;td&gt;   -0.222&lt;/td&gt; &lt;td&gt;    0.959&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x1&lt;/th&gt;         &lt;td&gt;    0.3973&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;  116.364&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.390&lt;/td&gt; &lt;td&gt;    0.404&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x2&lt;/th&gt; &lt;td&gt;    0.7946&lt;/td&gt; &lt;td&gt;    0.007&lt;/td&gt; &lt;td&gt;  116.364&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.781&lt;/td&gt; &lt;td&gt;    0.809&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 3.352&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   2.071&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.187&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   2.993&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt;-0.715&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;   0.224&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 2.407&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;7.31e+16&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;Warnings:&lt;br/&gt;[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.&lt;br/&gt;[2] The smallest eigenvalue is 8.01e-30. This might indicate that there are&lt;br/&gt;strong multicollinearity problems or that the design matrix is singular.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/e6f.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Neither does the popular machine learning package Scikit-Learn:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(X = data[[&amp;quot;x1&amp;quot;, &amp;quot;x2&amp;quot;]], y = data.y)
lm.coef_
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;array([0.39728922, 0.79457845])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;source: &lt;a href=&#34;https://knowyourmeme.com/photos/1250147-yamero&#34; target=&#34;_blank&#34;&gt;https://knowyourmeme.com/photos/1250147-yamero&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
